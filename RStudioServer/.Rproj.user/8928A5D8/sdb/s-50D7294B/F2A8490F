{
    "collab_server" : "",
    "contents" : "####################################################################################\n# Using R Server ScaleR and SparklyR interoperablity for data loading and analysis #\n#                                                                                  #\n# Data sets used are mtcars and sample small airline arrival delay data set from   #\n# RevoScaleR package SampleData                                                    #\n# Small airline sample data set has 600000 rows and 3 columns.                     #\n# \"ArrDelay\", \"CRSDepTime\", \"DayOfWeek\"                                            #\n####################################################################################\n\n\n# Load required libraries\nlibrary(RevoScaleR)\nlibrary(sparklyr)\nlibrary(dplyr)\n\n\n##\n## Use SparklyR to load and partition data and ScaleR for analytics\n##\n\n# Connect to Spark using rxSparkConnect, specifying 'interop = \"sparklyr\"'\n# this will create a sparklyr connection to spark, and allow you to use\n# dplyr for data manipulation. Using rxSparkConnect in this way will use\n# default values and rxOptions for creating a Spark connection, please\n# see \"?rxSparkConnect\" for how to define parameters specific to your setup\ncc <- rxSparkConnect(reset = TRUE, interop = \"sparklyr\")\n\n\n# The returned Spark connection (sc) provides a remote dplyr data source \n# to the Spark cluster using SparlyR within rxSparkConnect.\nsc <- rxGetSparklyrConnection(cc)\n\n\n# Next, load mtcars in to Spark using a dplyr pipeline\nmtcars_tbl <- copy_to(sc, mtcars)\n\n\n# Now, partition the data into Training and Test partitions using dplyr\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  mutate(cyl8 = cyl == 8) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 1099)\n\n\n# Register the partitions as DataFrames using sparklyr\nsdf_register(partitions$training, \"cars_training\")\nsdf_register(partitions$test, \"cars_test\")\n\n\n# Create a RxHiveData Object for each\ncars_training_hive <- RxHiveData(table = \"cars_training\", \n                                 colInfo = list(cyl = list(type = \"factor\")))\ncars_test_hive <- RxHiveData(table = \"cars_test\", \n                             colInfo = list(cyl = list(type = \"factor\")))\n\n\n# Use the Training set to train a model with rxLinMod()\nlmModel <- rxLinMod(mpg ~ wt + cyl, cars_training_hive)\n\n\n# Take a summary of the trained model (this step is optional)\nsummary(lmModel)\n\n\n# Currently, for rxPredict(), only XDF files are supported as an output\n# The following command will create the directory \"/user/RevoShare/MTCarsPredRes\" \n# to hold the composite XDF.\npred_res_xdf <- RxXdfData(\"/user/RevoShare/MTCarsPredRes\", fileSystem = RxHdfsFileSystem())\n\n\n# Run rxPredict, specifying the outData as our RxXdfData() object, write\n# the model variables into the results object so we can analyze out accuracy\n# after the prediction completes\npred_res_xdf <- rxPredict(lmModel, data = cars_test_hive, outData = pred_res_xdf, \n                          overwrite = TRUE, writeModelVars = TRUE)\n\n\n# Now, import the results from HDFS into a DataFrame so we can see our error\npred_res_df <- rxImport(pred_res_xdf)\n\n\n# Calculate the Root Mean Squared error of our prediction\nsqrt(mean((pred_res_df$mpg - pred_res_df$mpg_Pred)^2, na.rm = TRUE))\n\n\n# When you are finished, close the connection to Spark\nrxSparkDisconnect(cc)\n\n\n##\n## Use ScaleR to load the data and SparklyR for partitioning and analytics\n##\n\n\n# Connect to Spark using rxSparkConnect\ncc <- rxSparkConnect(reset = TRUE, interop = \"sparklyr\")\n\n\n# The returned Spark connection (sc) provides a remote dplyr data source \n# to the Spark cluster using SparlyR within rxSparkConnect.\nsc <- rxGetSparklyrConnection(cc)\n\n\n# Bind RxHdfsFileSystem to a variable, this will reduce code clutter\nhdfsFS <- RxHdfsFileSystem()\n\n\n# # # #\n# There are many data sources which we can use in MRS, in the this section\n# we have the option of 4 different file based data sources and how to import \n# data for use in Spark. If you wish to use any of these data sources, simply\n# comment out the desired data source line.\n#\n# One data source is XDF files stored in HDFS. Here we create an Data Object from\n# a composite XDF from HDFS, this can then be held in memory as a DataFrame, or \n# loaded into a Hive Table.\n#AirlineDemoSmall <- RxXdfData(file=\"/example/data/MRSSampleData/AirlineDemoSmallComposite\", fileSystem = hdfsFS)\n\n# Another option is CSV files stored in HDFS. To create a CSV Data Object from HDFS \n# we would use RxTextData(). This can also be used for othere plain text type formats\n#AirlineDemoSmall <- RxTextData(\"/example/data/MRSSampleData/AirlineDemoSmall.csv\", fileSystem = hdfsFS)\n\n# A third option is Parquet data using RxParquetData()\n#AirlineDemoSmall <- RxParquetData(\"/example/data/MRSSampleData/AirlineDemoSmallParquet\", fileSystem = hdfsFS)\n\n#Lastly, ORC  Data using RxOrcData()\n#AirlineDemoSmall <- RxOrcData(\"/example/data/MRSSampleData/AirlineDemoSmallOrc\", fileSystem = hdfsFS)\n# # # #\n\n\n# Regardless of which data source used, to work with it using dplyr, we need to \n# write it to a Hive table, so, Next, create a Hive Data Object using RxHiveData()\nAirlineDemoSmallHive <- RxHiveData(table=\"AirlineDemoSmall\")\n\n\n# Use rxDataStep to load the data into the table\n# this depends on data\nrxDataStep(inData = AirlineDemoSmall, outFile = AirlineDemoSmallHive,\n           overwrite = TRUE) # takes about 90 seconds on 1 node cluster\n\n\n###\n#\n# If you wanted the data as a data frame in Spark, you would use\n# rxDataStep() like so:\n#\n# AirlineDemoSmalldf <- rxDataStep(inData = AirlineDemoSmall, \n#            overwrite = TRUE) \n#\n# But data must be in a Hive Table for use with dplyr as an In-Spark \n# object\n###\n\n\n# To see that the table was created list all Hive tables\nsrc_tbls(sc)\n\n\n# Next, define a dplyr data source referencing the Hive table\n# This caches the data in Spark\ntbl_cache(sc, \"airlinedemosmall\")\nflights_tbl <- tbl(sc, \"airlinedemosmall\")\n\n\n# Print out a few rows of the table\nflights_tbl\n\n\n# Filter the data to remove missing observations\nmodel_data <- flights_tbl %>%\n  filter(!is.na(ArrDelay)) %>%\n  select(ArrDelay, CRSDepTime, DayOfWeek)\n\n\n# Now, partition data into training and test sets using dplyr\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, test = 0.2, seed = 6666)\n\n\n# Fit a linear model using Spark's ml_linear_regression to attempt\n# to predict CRSDepTime by Day of Week and Delay\nml1 <- model_partition$train %>%\n  ml_linear_regression(CRSDepTime ~ DayOfWeek + ArrDelay)\n\n\n# Run a summary on the model to see the estimated CRSDepTime per Day of Week\nsummary(ml1)\n\n\n# When you are finished, close the connection to Spark\nrxSparkDisconnect(cc)",
    "created" : 1503504976719.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2543451343",
    "id" : "F2A8490F",
    "lastKnownWriteTime" : 1503513477,
    "last_content_update" : 1503513477039,
    "path" : "~/RStudioServer/02-ScaleR-SparklyR.R",
    "project_path" : "02-ScaleR-SparklyR.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}